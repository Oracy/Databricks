# Learning objectives

1. Describe how Delta Lake fits into the Databricks Lakehouse Platform.
2. Explain the four elements encompassed by Delta Lake.  
3. Summarize Delta Lake functionality that helps organizations solve common challenges related to enterprise-scale data analytics.
4. Articulate examples of how real-world organizations have employed Delta Lake on Databricks to improve business outcomes.

---

1. R: Once data lands into an open data lake, Delta Lake is used to prepare that data for data engineering, business intelligence, SQL analytics, data science, and machine learning use-cases.
Bronze tables:
    - Contain raw data ingested from various sources (JSON files, RDBMS data, IoT data).
    - Is often kept for years and acn be saves "as-is"
Silver tables:
    - This data is directly queryable and ready for big data and AI projects.
    - Is often joined from various bronze tables to enrich records or update records based on recent activity.
    - Clean, Normalized, and referred to as your single source of truth.
Gold tables:
    - Provide business-level aggregates used for particular use-cases.
    - Tables are usually related to one particular business objective.

2. R:
    - Delta Files
        - By design, Delta Lake uses Parquet files to store an organization’s data in their object storage, but have an additional layer over them. This additional layer tracks data versioning and metadata, stores transaction logs to keep track of changes made to a data table or object storage directory, and provides ACID transactions. 
    - Delta Tables
        - Is a collection of data kept using the Delta Lake technology and consists of three things:
            1. Delta files containing the data and kept in object storage
            2. A Delta table registered in a Metastore (a metastore is simply a catalog that tracks your data’s metadata - data about your data)
            3. the Delta Transaction Log saved with Delta files in object storage
    - Delta Optimization Engine
        - Delta Engine accelerates data lake operations and supports a variety of workloads ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of these optimizations take place automatically; you get the benefits of these Delta Engine capabilities just by using Databricks for your data lakes.
    - Delta Lake Storage Layer
        - Your organization stores its data in a Delta Lake Storage Layer and then accesses that data via Databricks. A key idea here is that an organization keeps all of this data in files in object storage. This is beneficial because it means your data is kept in a lower-cost and easily scalable environment.

3. R:
    - ACID Transactions (Atomicity, Consistency, Isolation, Durability)
    - Schema Management
        - Delta Lake gives you the ability to specify and enforce your data schema. It automatically validates that the schema of the data being written is compatible with the schema of the table it is being written into. Columns that are present in the table but not in the data are set to null
    - Scalable Metadata Handling
        - Big data is very large in size, and its metadata (the information about the files containing the data and the nature of the data) can be very large as well. With Delta Lake, metadata is processed just like regular data - with distributed processing. 
    - Unified Batch and Streaming Data
        - Delta Lake is designed from the ground up to allow a single system to support both batch and stream processing of data. The transactional guarantees of Delta Lake mean that each micro-batch transaction creates a new version of a table that is instantly available for insights. Many Databricks users use Delta Lake to transform the update frequency of their dashboards and reports from days to minutes while eliminating the need for multiple systems.
    - Data Versioning and Time Travel
        - With Delta Lake, the transaction logs used to ensure ACID compliance create an auditable history of every version of the table, indicating which files have changed between versions. This log makes it easy to retain historical versions of the data to fulfill compliance requirements in various industries such as GDPR and CCPA.

4. R: Read PDFs on Kindle
