# Learning objectives

1. Describe the basic overview of Databricks Machine Learning.
2. Identify how using Databricks Machine Learning benefits data science and machine learning teams.
3. Summarize the fundamental components and functionalities of Databricks Machine Learning.
4. Exemplify successful use cases of Databricks Machine Learning by real Databricks customers.

---

1. R:
    - Production machine learning depends on code and data.
        - The Databricks ML platform is data-native. This means that it sits in the exact same place as your data, so there’s no need to move your data from one place or another.
        - Databricks ML supports easy integration for new and existing machine learning projects with Databricks Repos.
    - Machine learning requires many different roles to get involved.
        - The Databricks ML platform sits within the larger Databricks platform. This includes environments friendly to data engineering and SQL analysis, so the whole team can get involved in the same platform using the same data.
        - Databricks ML supports both machine learning development and production workflows, so data scientists and machine learning engineers can work together in the same place.
    - Machine learning requires integrating many different components.
        - The Databricks ML platform includes tools to support the entire machine learning workflow. From feature organization to model serving and scoring, the tools in Databricks ML have you covered.
        - All of these components come packaged in a tested platform and runtime, which makes architecting systems for individual projects easier.
    1. Data Quality (Data Storage)
        - DE are operating on the same platform, they're able to work together with ML teams to ensure data is of the highest quality.
    2. Compute Resources (Environment Selection)
        - Databricks offers autoscaling for compute resources to ensure that practitioners are using the exact amount of resources they need
    3. Feature Development (Training Data Preparation)
        - Newer Databricks feature, Feature Store. ML, data teams re able to roganize their computed features in a clean and centralized feature registry
    4. Model Development
        - Databricks AutoML, gets you started by building a series of models and logging the Run results to Experiments with MLflow.
    5. Machine learning operations (Model Serving and Deployment)
        - MLflow Model Registry provides a centralized place to store and manage your machine learning models.
    6. Governance and security
        - Databricks Tools are abel to be restricted with access control lists (ACLs)
    7. Automation
        - ML practitioners are able to schedule parts of their project to run on a specific frequency to take advantage of all of the benefits of Databricks ML on a regular, automated basis.
2. R:

3. R: MLFlow, Feature Store, AutoML

4. R:

---

## Basics of MLflow

- MLflow Tracking: Allows you to track your model runs within experiments to record and compare results.
- MLflow Models: Allow you to manage and deploy models from a variety of ML libraries to a variety of model serving and inference platforms.
- MLflow Projects: Allow you to package ML code in a reusable, reproducible form to share with other data scientists or transfer to production.
- MLflow Model Registry: Allows you to centralize a model store for managing models’ full lifecycle stage transitions: from staging to production, with capabilities for versioning and annotating.
- MLflow Model Serving: Allows you to host MLflow Models for real-time serving.

## Basics of AutoML

- When using the AutoML UI, users on Databricks ML will be able to manually create an AutoML experiment, select the appropriate cluster, set up a classification or regression problem, and select the label variables attempting to be predicted. This ML-friendly user interface is ideal for exploration stages early in machine learning application development because practitioners can quickly run a lot of models to get a decent idea of what type of model will best work. Then, they can utilize the generated code moving forward and add custom tasks like feature engineering.

## Feature Store

- Feature Store is a centralized repository and registry for features, or input variables, to machine learning models.
- Feature Store inherits all of Delta Lake’s benefits:
  - Open storage format
  - Build-in data versioning and governance
  - Native access through PySpark, SQL, etc.
