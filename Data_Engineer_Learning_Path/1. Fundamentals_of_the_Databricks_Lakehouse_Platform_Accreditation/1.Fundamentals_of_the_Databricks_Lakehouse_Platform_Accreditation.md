# Learning objectives

1. Describe what the Databricks Lakehouse Platform is. Describe fundamental concepts that define what the Databricks Lakehouse Platform is.
2. Explain the origins of the Lakehouse data management paradigm.
3. Outline fundamental problems that cause most enterprises to struggle with managing and making use of their data.
4. Identify the most popular components of the Databricks Lakehouse Platform used by data practitioners, depending on their unique role.
5. Give examples of organizations that have used the Databricks Lakehouse Platform to streamline big data processing and analytics.
6. What does Databricks help organizations do? 
7. Describe security features that come built-in to the Databricks Lakehouse Platform.

---

1. R: It is a SaaS that makes big data and AI easier for organizations to manage, enabling data-driven innovation in all enterprises, empower everyone on a data science team to work together, in one secure platform.
Databricks offers a Lakehouse Platform, accessible via an online login, that allows data science teams to collaborate on their work. This Lakehouse Platform gives customers access to Databricks-native tools,  managed open-source tools, and technical resources to help customers along the way.
As a platform focused on computation and analytics, Databricks seeks to help our customers make choices that unlock new opportunities, reduce redundancies, and connect data teams. That is why we created the Lakehouse

2. R: The Lakehouse data management paradigm originated as Databricks tried to help organizations solve their big data and AI challenges.

3. R:
    - working with big data is not easy
        - Where/how will we store our big data?
        - How can we process batch and stream data?
        - How can we use different types of data together in our analyses (unstructured vs. structured data)?
        - How can we keep track of all of the work we’re doing on our big data?
    - Siloed roles lead to organizational inefficiencies.
    - Data security challenges

4. R:
    1. Databricks Lakehouse Platform for Data Engineering: Delta Lake
        - ACID transactions, which are database transaction properties that guarantee data validity. With ACID transactions, you don’t have to worry about missing data or inconsistencies in your data from interrupted or deleted operational transactions because changes to your data are performed as if they are a single operation. 
        - Indexing, which is a way to get an unordered table (which might be inefficient to query) into an order that will maximize the efficiency of your queries
        - Table access control lists (ACLs), or governance mechanisms that ensure that only users who should have access to data can access it
        - Expectation-setting, which refers to the ability for you to configure Delta Lake based on your workload patterns and business needs

    2. Databricks Lakehouse Platform for BI & SQL Analytics: Databricks SQL
        - Databricks SQL provides SQL users a SQL-native Interface to write queries that explore their organization’s Delta Lake table. Regularly used SQL code can be saved as snippets for quick reuse, and query results can be cached to keep the query short.

    3. Databricks Lakehouse Platform for Data Science & Machine Learning: Databricks Machine Learning
        - Databricks ML was created for machine learning teams to explore data, prepare and process data, build and test machine learning models, deploy those models, and optimize them. In other words, it is the part of the Databricks Lakehouse Platform tailored for machine learning teams to work on their projects throughout the entire machine learning lifecycle.

    - Transaction support to ensure that multiple parties can concurrently read or write data
    - Data schema enforcement to ensure data integrity (writes to a table are rejected if they do not match the table’s schema)
    - Governance and auditioning mechanisms to make sure you can see how data is being used 
    - BI support so that BI tools can work directly on source data - this reduces data staleness.
    - Storage is decoupled from compute, which means that it is easier for your system to scale to more concurrent users and data sizes.
    - Openness - Storage formats used are open and standard. Plus, APIs and various other tools make it easy for team members to access data directly.
    - Support for all data types - structured, unstructured, semi-structured
    - End-to-end streaming so that real-time reporting and real-time data can be integrated into data analytics processes just as existing data is
    - Support for diverse workloads, including data engineering, data science, machine learning, and SQL analytics - all on the same data repository.

5. R: Read PDFs on Kindle

6. R:
    - Ingest, process, and transform massive quantities and types of data
    - Explore data through data science techniques, including but not limited to machine learning
    - Guarantee that data available for business queries is reliable and up to date
    - Provide data engineers, data scientists, and data analysts the unique tools they need to do their work
    - Overcome traditional challenges associated with data science and machine learning workflows (we will explore this in detail in our next lesson)

7. R:
    - Keeping data safe as an organization scales
    - Ensuring privacy as data practitioners carry out their workflows 
    - Meeting compliance and certification regulations

    - Customer-managed VPC/VNET - these are private networks that can be deployed with custom network configurations to comply with internal cloud and data governance policies (as well as adhere to external regulations).
    - Access Control Lists / IP Whitelisting: This enables the use of strict IP access lists to specify which connections can or cannot be made in and out of an organization's workspaces, minimizing the attack surface. With access control lists / IP Whitelisting, all incoming access to an organization's Databricks Web application and REST APIs require users to connect from an authorized IP address or VPN.
    - Code Isolation: This means that data practitioners can run their data analytics workflows on the same computational resources while ensuring that each user only has access to the data they are authorized to access. 
    - Private Network between Data and Control Planes: The Databricks Lakehouse Platform is structured to enable secure cross-functional team collaboration while keeping a significant amount of backend services managed by Databricks. Databricks operates out of a control plane and a data plane. The control plane includes the backend services that Databricks manages in its own cloud account. The data plane is managed by an organization's cloud account and is where data resides and is processed. With private networks between data and control planes, all communications between control and data planes happen through private networks (they are not sent over public networks).

    - Databricks certifications and compliance attestations include:
        - SOC 2 Type II
        - ISO 27018
        - ISO 27001
        - HIPAA
        - GDPR | Read our FAQ
        - FedRAMP (Azure)
        - PCI DSS (AWS)

---

Delta Lake for data engineering
Databricks SQL for BI and SQL analytics
Databricks Machine Learning for data science and machine learning 